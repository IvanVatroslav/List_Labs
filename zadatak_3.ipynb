{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T17:38:27.011781Z",
     "start_time": "2024-09-08T17:38:26.995826Z"
    }
   },
   "id": "251d0f1dd9b8e7bb",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      Red   Green   Blue     NIR      ndvi      ndwi    msavi2     mtvi2  \\\n0  1376.0  1150.0  795.0  2598.0  0.307499 -0.386339  0.470314  0.338574   \n1  1342.0  1101.0  776.0  2676.0  0.332006 -0.416998  0.498459  0.360913   \n2  1347.0  1101.0  785.0  2671.0  0.329517 -0.416225  0.495648  0.356393   \n3  1303.0  1140.0  722.0  2628.0  0.337064 -0.394904  0.504138  0.393367   \n4  1358.0  1141.0  727.0  2758.0  0.340136 -0.414722  0.507569  0.380207   \n\n       vari      tgi  Class  \n0 -0.130560  13390.0      0  \n1 -0.144571  11065.0      0  \n2 -0.147925  10350.0      0  \n3 -0.094712  19375.0      0  \n4 -0.122460  17245.0      0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Red</th>\n      <th>Green</th>\n      <th>Blue</th>\n      <th>NIR</th>\n      <th>ndvi</th>\n      <th>ndwi</th>\n      <th>msavi2</th>\n      <th>mtvi2</th>\n      <th>vari</th>\n      <th>tgi</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1376.0</td>\n      <td>1150.0</td>\n      <td>795.0</td>\n      <td>2598.0</td>\n      <td>0.307499</td>\n      <td>-0.386339</td>\n      <td>0.470314</td>\n      <td>0.338574</td>\n      <td>-0.130560</td>\n      <td>13390.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1342.0</td>\n      <td>1101.0</td>\n      <td>776.0</td>\n      <td>2676.0</td>\n      <td>0.332006</td>\n      <td>-0.416998</td>\n      <td>0.498459</td>\n      <td>0.360913</td>\n      <td>-0.144571</td>\n      <td>11065.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1347.0</td>\n      <td>1101.0</td>\n      <td>785.0</td>\n      <td>2671.0</td>\n      <td>0.329517</td>\n      <td>-0.416225</td>\n      <td>0.495648</td>\n      <td>0.356393</td>\n      <td>-0.147925</td>\n      <td>10350.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1303.0</td>\n      <td>1140.0</td>\n      <td>722.0</td>\n      <td>2628.0</td>\n      <td>0.337064</td>\n      <td>-0.394904</td>\n      <td>0.504138</td>\n      <td>0.393367</td>\n      <td>-0.094712</td>\n      <td>19375.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1358.0</td>\n      <td>1141.0</td>\n      <td>727.0</td>\n      <td>2758.0</td>\n      <td>0.340136</td>\n      <td>-0.414722</td>\n      <td>0.507569</td>\n      <td>0.380207</td>\n      <td>-0.122460</td>\n      <td>17245.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original = pd.read_csv('training_data.csv')\n",
    "df_original.head()"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-08T18:39:53.128968Z",
     "start_time": "2024-09-08T18:39:53.045162Z"
    }
   },
   "id": "initial_id",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "          Red   Green    Blue     NIR      ndvi      ndwi    msavi2     mtvi2  \\\n0      1376.0  1150.0   795.0  2598.0  0.307499 -0.386339  0.470314  0.338574   \n1      1342.0  1101.0   776.0  2676.0  0.332006 -0.416998  0.498459  0.360913   \n2      1347.0  1101.0   785.0  2671.0  0.329517 -0.416225  0.495648  0.356393   \n3      1303.0  1140.0   722.0  2628.0  0.337064 -0.394904  0.504138  0.393367   \n4      1358.0  1141.0   727.0  2758.0  0.340136 -0.414722  0.507569  0.380207   \n...       ...     ...     ...     ...       ...       ...       ...       ...   \n48137  2493.0  2214.0  1764.0  2726.0  0.044645 -0.103644  0.085459 -0.022867   \n48138  2517.0  2298.0  1835.0  2722.0  0.039130 -0.084462  0.075299 -0.010665   \n48139  2452.0  2291.0  1868.0  2724.0  0.052550 -0.086341  0.099837  0.032247   \n48140  2285.0  2130.0  1642.0  2595.0  0.063525 -0.098413  0.119440  0.049287   \n48141  2371.0  2207.0  1749.0  2678.0  0.060804 -0.096418  0.114619  0.043473   \n\n           vari      tgi  Class  \n0     -0.130560  13390.0      0  \n1     -0.144571  11065.0      0  \n2     -0.147925  10350.0      0  \n3     -0.094712  19375.0      0  \n4     -0.122460  17245.0      0  \n...         ...      ...    ...  \n48137 -0.094801  17235.0      2  \n48138 -0.073490  20115.0      2  \n48139 -0.056000  19745.0      2  \n48140 -0.055896  23855.0      2  \n48141 -0.057971  21740.0      2  \n\n[48142 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Red</th>\n      <th>Green</th>\n      <th>Blue</th>\n      <th>NIR</th>\n      <th>ndvi</th>\n      <th>ndwi</th>\n      <th>msavi2</th>\n      <th>mtvi2</th>\n      <th>vari</th>\n      <th>tgi</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1376.0</td>\n      <td>1150.0</td>\n      <td>795.0</td>\n      <td>2598.0</td>\n      <td>0.307499</td>\n      <td>-0.386339</td>\n      <td>0.470314</td>\n      <td>0.338574</td>\n      <td>-0.130560</td>\n      <td>13390.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1342.0</td>\n      <td>1101.0</td>\n      <td>776.0</td>\n      <td>2676.0</td>\n      <td>0.332006</td>\n      <td>-0.416998</td>\n      <td>0.498459</td>\n      <td>0.360913</td>\n      <td>-0.144571</td>\n      <td>11065.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1347.0</td>\n      <td>1101.0</td>\n      <td>785.0</td>\n      <td>2671.0</td>\n      <td>0.329517</td>\n      <td>-0.416225</td>\n      <td>0.495648</td>\n      <td>0.356393</td>\n      <td>-0.147925</td>\n      <td>10350.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1303.0</td>\n      <td>1140.0</td>\n      <td>722.0</td>\n      <td>2628.0</td>\n      <td>0.337064</td>\n      <td>-0.394904</td>\n      <td>0.504138</td>\n      <td>0.393367</td>\n      <td>-0.094712</td>\n      <td>19375.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1358.0</td>\n      <td>1141.0</td>\n      <td>727.0</td>\n      <td>2758.0</td>\n      <td>0.340136</td>\n      <td>-0.414722</td>\n      <td>0.507569</td>\n      <td>0.380207</td>\n      <td>-0.122460</td>\n      <td>17245.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48137</th>\n      <td>2493.0</td>\n      <td>2214.0</td>\n      <td>1764.0</td>\n      <td>2726.0</td>\n      <td>0.044645</td>\n      <td>-0.103644</td>\n      <td>0.085459</td>\n      <td>-0.022867</td>\n      <td>-0.094801</td>\n      <td>17235.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>48138</th>\n      <td>2517.0</td>\n      <td>2298.0</td>\n      <td>1835.0</td>\n      <td>2722.0</td>\n      <td>0.039130</td>\n      <td>-0.084462</td>\n      <td>0.075299</td>\n      <td>-0.010665</td>\n      <td>-0.073490</td>\n      <td>20115.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>48139</th>\n      <td>2452.0</td>\n      <td>2291.0</td>\n      <td>1868.0</td>\n      <td>2724.0</td>\n      <td>0.052550</td>\n      <td>-0.086341</td>\n      <td>0.099837</td>\n      <td>0.032247</td>\n      <td>-0.056000</td>\n      <td>19745.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>48140</th>\n      <td>2285.0</td>\n      <td>2130.0</td>\n      <td>1642.0</td>\n      <td>2595.0</td>\n      <td>0.063525</td>\n      <td>-0.098413</td>\n      <td>0.119440</td>\n      <td>0.049287</td>\n      <td>-0.055896</td>\n      <td>23855.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>48141</th>\n      <td>2371.0</td>\n      <td>2207.0</td>\n      <td>1749.0</td>\n      <td>2678.0</td>\n      <td>0.060804</td>\n      <td>-0.096418</td>\n      <td>0.114619</td>\n      <td>0.043473</td>\n      <td>-0.057971</td>\n      <td>21740.0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>48142 rows Ã— 11 columns</p>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T18:39:56.795818Z",
     "start_time": "2024-09-08T18:39:56.762875Z"
    }
   },
   "id": "facdf7e74815fb25",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Red       0\nGreen     0\nBlue      0\nNIR       0\nndvi      0\nndwi      0\nmsavi2    0\nmtvi2     0\nvari      0\ntgi       0\nClass     0\ndtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T17:38:27.860509Z",
     "start_time": "2024-09-08T17:38:27.834579Z"
    }
   },
   "id": "503d6f3233c09fb4",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48142 entries, 0 to 48141\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Red     48142 non-null  float64\n",
      " 1   Green   48142 non-null  float64\n",
      " 2   Blue    48142 non-null  float64\n",
      " 3   NIR     48142 non-null  float64\n",
      " 4   ndvi    48142 non-null  float64\n",
      " 5   ndwi    48142 non-null  float64\n",
      " 6   msavi2  48142 non-null  float64\n",
      " 7   mtvi2   48142 non-null  float64\n",
      " 8   vari    48142 non-null  float64\n",
      " 9   tgi     48142 non-null  float64\n",
      " 10  Class   48142 non-null  int64  \n",
      "dtypes: float64(10), int64(1)\n",
      "memory usage: 4.0 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T17:38:28.404356Z",
     "start_time": "2024-09-08T17:38:28.376431Z"
    }
   },
   "id": "cc1be779ded6ea98",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1500x1000 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 1500x1000 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Identify numeric features (excluding the 'Class' column)\n",
    "features = df_original.columns.difference(['Class'])\n",
    "\n",
    "# ===== Figure 1: Normal Distributions =====\n",
    "\n",
    "# Create a figure with subplots for normal distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "df[features].hist(figsize=(15, 10), bins=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig('normal_feature_distributions.png')\n",
    "plt.close()\n",
    "\n",
    "# ===== Figure 2: Log Transformed Distributions =====\n",
    "\n",
    "# Create a copy of the DataFrame to apply log transformation\n",
    "df_log = df_original.copy()\n",
    "\n",
    "# Apply log transformation with shifting to each feature\n",
    "for column in features:\n",
    "    min_value = df_log[column].min()\n",
    "    if min_value <= 0:\n",
    "        shift = abs(min_value) + 1  # Shift all values by |min_value| + 1\n",
    "        df_log[column] = np.log(df_log[column] + shift)\n",
    "    else:\n",
    "        df_log[column] = np.log(df_log[column])\n",
    "\n",
    "# Create a figure with subplots for log-transformed distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "df_log[features].hist(figsize=(15, 10), bins=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig('log_transformed_feature_distributions.png')\n",
    "plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T17:51:14.871576Z",
     "start_time": "2024-09-08T17:51:09.737993Z"
    }
   },
   "id": "265eef79ae44daaa",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c2e2cf1f6f490937"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df=df_original.copy()\n",
    "\n",
    "# Assume df is your original DataFrame\n",
    "X_normal = df.drop(columns=['Class'])  # Normal Data Features\n",
    "y_normal = df['Class']  # Labels\n",
    "\n",
    "# ===== Step 1: Split Normal Data =====\n",
    "# First split for Normal Data (Training 70%, Temp 30%)\n",
    "X_train_normal, X_temp_normal, y_train_normal, y_temp_normal = train_test_split(X_normal, y_normal, test_size=0.3, stratify=y_normal, random_state=42)\n",
    "\n",
    "# Second split for Normal Data (Validation 15%, Test 15%)\n",
    "X_val_normal, X_test_normal, y_val_normal, y_test_normal = train_test_split(X_temp_normal, y_temp_normal, test_size=0.5, stratify=y_temp_normal, random_state=42)\n",
    "\n",
    "# ===== Step 2: Log Transformation =====\n",
    "# Log-Transform the data (with shifting to handle non-positive values)\n",
    "df_log = df.copy()\n",
    "features = df.columns.difference(['Class'])\n",
    "\n",
    "for column in features:\n",
    "    min_value = df_log[column].min()\n",
    "    if min_value <= 0:\n",
    "        shift = abs(min_value) + 1\n",
    "        df_log[column] = np.log(df_log[column] + shift)\n",
    "    else:\n",
    "        df_log[column] = np.log(df_log[column])\n",
    "\n",
    "X_log = df_log.drop(columns=['Class'])  # Log-Transformed Data Features\n",
    "y_log = df_log['Class']  # Labels (same as original)\n",
    "\n",
    "# ===== Step 3: Split Log-Transformed Data =====\n",
    "# First split for Log-Transformed Data (Training 70%, Temp 30%)\n",
    "X_train_log, X_temp_log, y_train_log, y_temp_log = train_test_split(X_log, y_log, test_size=0.3, stratify=y_log, random_state=42)\n",
    "\n",
    "# Second split for Log-Transformed Data (Validation 15%, Test 15%)\n",
    "X_val_log, X_test_log, y_val_log, y_test_log = train_test_split(X_temp_log, y_temp_log, test_size=0.5, stratify=y_temp_log, random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T18:28:09.596212Z",
     "start_time": "2024-09-08T18:28:09.449564Z"
    }
   },
   "id": "949e60161a75b4e5",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=df_original.copy()\n",
    "\n",
    "# List of features to log-transform\n",
    "log_features = ['Blue', 'Green', 'NIR', 'Red']  # Adjust based on your column names\n",
    "normal_features = df.columns.difference(['Class', 'Blue', 'Green', 'NIR', 'Red'])\n",
    "\n",
    "# Create a copy of the DataFrame\n",
    "df_mixed = df.copy()\n",
    "\n",
    "# Apply log transformation to the selected features\n",
    "for column in log_features:\n",
    "    min_value = df_mixed[column].min()\n",
    "    if min_value <= 0:\n",
    "        shift = abs(min_value) + 1  # Shift values if any non-positive numbers are present\n",
    "        df_mixed[column] = np.log(df_mixed[column] + shift)\n",
    "    else:\n",
    "        df_mixed[column] = np.log(df_mixed[column])\n",
    "\n",
    "# Now df_mixed contains log-transformed 'Blue', 'Green', 'NIR', 'Red' and normal rest\n",
    "# Features (X) and target (y)\n",
    "X_mixed = df_mixed.drop(columns=['Class'])  # Mixed (log and normal) features\n",
    "y_mixed = df_mixed['Class']  # Labels\n",
    "\n",
    "# First split: Training set (70%) and temp set (30%)\n",
    "X_train_mixed, X_temp_mixed, y_train_mixed, y_temp_mixed = train_test_split(X_mixed, y_mixed, test_size=0.3, stratify=y_mixed, random_state=42)\n",
    "\n",
    "# Second split: Validation (15%) and Test (15%) from the temp set\n",
    "X_val_mixed, X_test_mixed, y_val_mixed, y_test_mixed = train_test_split(X_temp_mixed, y_temp_mixed, test_size=0.5, stratify=y_temp_mixed, random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T18:49:37.236373Z",
     "start_time": "2024-09-08T18:49:37.210416Z"
    }
   },
   "id": "e2fc8c361cfb439b",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ac9488f4b9639ae0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T18:40:17.206133Z",
     "start_time": "2024-09-08T18:40:17.150312Z"
    }
   },
   "id": "200da5a9c505d98f",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Data - Validation Set Evaluation\n",
      "[[1034    0    7]\n",
      " [   0 3833    0]\n",
      " [   3    0 2344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "\n",
      "Normal Data - Test Set Evaluation\n",
      "[[1040    0    1]\n",
      " [   0 3833    0]\n",
      " [   4    0 2344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Train Random Forest on Normal Data\n",
    "model_normal = RandomForestClassifier(random_state=42)\n",
    "model_normal.fit(X_train_normal, y_train_normal)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_normal = model_normal.predict(X_val_normal)\n",
    "print(\"Normal Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_normal, y_pred_val_normal))\n",
    "print(classification_report(y_val_normal, y_pred_val_normal))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_normal = model_normal.predict(X_test_normal)\n",
    "print(\"Normal Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_normal, y_pred_test_normal))\n",
    "print(classification_report(y_test_normal, y_pred_test_normal))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T18:32:39.870017Z",
     "start_time": "2024-09-08T18:32:31.557726Z"
    }
   },
   "id": "1f57cced5049e659",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bcdab4341821800c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Transformed Data - Validation Set Evaluation\n",
      "[[1034    0    7]\n",
      " [   0 3833    0]\n",
      " [   3    0 2344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "\n",
      "Log-Transformed Data - Test Set Evaluation\n",
      "[[1040    0    1]\n",
      " [   0 3833    0]\n",
      " [   4    0 2344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest on Log-Transformed Data\n",
    "model_log = RandomForestClassifier(random_state=42)\n",
    "model_log.fit(X_train_log, y_train_log)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_log = model_log.predict(X_val_log)\n",
    "print(\"Log-Transformed Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_log, y_pred_val_log))\n",
    "print(classification_report(y_val_log, y_pred_val_log))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_log = model_log.predict(X_test_log)\n",
    "print(\"Log-Transformed Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_log, y_pred_test_log))\n",
    "print(classification_report(y_test_log, y_pred_test_log))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T18:33:06.431894Z",
     "start_time": "2024-09-08T18:32:58.768363Z"
    }
   },
   "id": "5222d6ff7e29e115",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T18:09:38.410333Z",
     "start_time": "2024-09-08T18:09:38.394375Z"
    }
   },
   "id": "9f18f88f366caec8",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a978172c38f4ecf9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ca3f8b8478d0720"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed Data (Log for Blue, Green, NIR, Red) - Validation Set Evaluation\n",
      "[[1034    0    7]\n",
      " [   0 3833    0]\n",
      " [   3    0 2344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "\n",
      "Mixed Data (Log for Blue, Green, NIR, Red) - Test Set Evaluation\n",
      "[[1040    0    1]\n",
      " [   0 3833    0]\n",
      " [   4    0 2344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Train Random Forest on the mixed (log and normal) data\n",
    "model_mixed = RandomForestClassifier(random_state=42)\n",
    "model_mixed.fit(X_train_mixed, y_train_mixed)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_mixed = model_mixed.predict(X_val_mixed)\n",
    "print(\"Mixed Data (Log for Blue, Green, NIR, Red) - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_mixed, y_pred_val_mixed))\n",
    "print(classification_report(y_val_mixed, y_pred_val_mixed))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_mixed = model_mixed.predict(X_test_mixed)\n",
    "print(\"Mixed Data (Log for Blue, Green, NIR, Red) - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_mixed, y_pred_test_mixed))\n",
    "print(classification_report(y_test_mixed, y_pred_test_mixed))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T18:40:48.914007Z",
     "start_time": "2024-09-08T18:40:41.183971Z"
    }
   },
   "id": "8b8273ffbb6695c7",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Scaled Normal Data - Validation Set Evaluation\n",
      "[[1030    0   11]\n",
      " [   0 3833    0]\n",
      " [  14    3 2330]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      0.99      0.99      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       0.99      0.99      0.99      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "\n",
      "Logistic Regression - Scaled Normal Data - Test Set Evaluation\n",
      "[[1036    0    5]\n",
      " [   0 3833    0]\n",
      " [  10    0 2338]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_normal_scaled = scaler.fit_transform(X_train_normal)\n",
    "X_val_normal_scaled = scaler.transform(X_val_normal)\n",
    "X_test_normal_scaled = scaler.transform(X_test_normal)\n",
    "\n",
    "# Train Logistic Regression on scaled normal data\n",
    "model_lr_normal = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model_lr_normal.fit(X_train_normal_scaled, y_train_normal)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_lr_normal = model_lr_normal.predict(X_val_normal_scaled)\n",
    "print(\"Logistic Regression - Scaled Normal Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_normal, y_pred_val_lr_normal))\n",
    "print(classification_report(y_val_normal, y_pred_val_lr_normal))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_lr_normal = model_lr_normal.predict(X_test_normal_scaled)\n",
    "print(\"Logistic Regression - Scaled Normal Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_normal, y_pred_test_lr_normal))\n",
    "print(classification_report(y_test_normal, y_pred_test_lr_normal))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:17:55.700002Z",
     "start_time": "2024-09-08T19:17:55.022841Z"
    }
   },
   "id": "bee0ad1bac720a31",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Scaled Log-Transformed Data - Validation Set Evaluation\n",
      "[[1031    0   10]\n",
      " [   0 3833    0]\n",
      " [  14    6 2327]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      0.99      0.99      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       0.99      0.99      0.99      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "\n",
      "Logistic Regression - Scaled Log-Transformed Data - Test Set Evaluation\n",
      "[[1035    0    6]\n",
      " [   0 3833    0]\n",
      " [   9    0 2339]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Scale the log-transformed data\n",
    "scaler_log = StandardScaler()\n",
    "X_train_log_scaled = scaler_log.fit_transform(X_train_log)\n",
    "X_val_log_scaled = scaler_log.transform(X_val_log)\n",
    "X_test_log_scaled = scaler_log.transform(X_test_log)\n",
    "\n",
    "# Train Logistic Regression on scaled log-transformed data\n",
    "model_lr_log = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model_lr_log.fit(X_train_log_scaled, y_train_log)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_lr_log = model_lr_log.predict(X_val_log_scaled)\n",
    "print(\"Logistic Regression - Scaled Log-Transformed Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_log, y_pred_val_lr_log))\n",
    "print(classification_report(y_val_log, y_pred_val_lr_log))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_lr_log = model_lr_log.predict(X_test_log_scaled)\n",
    "print(\"Logistic Regression - Scaled Log-Transformed Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_log, y_pred_test_lr_log))\n",
    "print(classification_report(y_test_log, y_pred_test_lr_log))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:18:49.025210Z",
     "start_time": "2024-09-08T19:18:48.100726Z"
    }
   },
   "id": "7782b234fa24ec01",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Scaled Mixed Data - Validation Set Evaluation\n",
      "[[1029    0   12]\n",
      " [   0 3833    0]\n",
      " [  14    5 2328]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       0.99      0.99      0.99      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       0.99      0.99      0.99      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "\n",
      "Logistic Regression - Scaled Mixed Data - Test Set Evaluation\n",
      "[[1035    0    6]\n",
      " [   0 3833    0]\n",
      " [  10    0 2338]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "# Scale the mixed data (some features log-transformed, some normal)\n",
    "scaler_mixed = StandardScaler()\n",
    "X_train_mixed_scaled = scaler_mixed.fit_transform(X_train_mixed)\n",
    "X_val_mixed_scaled = scaler_mixed.transform(X_val_mixed)\n",
    "X_test_mixed_scaled = scaler_mixed.transform(X_test_mixed)\n",
    "\n",
    "# Train Logistic Regression on scaled mixed data\n",
    "model_lr_mixed = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model_lr_mixed.fit(X_train_mixed_scaled, y_train_mixed)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_lr_mixed = model_lr_mixed.predict(X_val_mixed_scaled)\n",
    "print(\"Logistic Regression - Scaled Mixed Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_mixed, y_pred_val_lr_mixed))\n",
    "print(classification_report(y_val_mixed, y_pred_val_lr_mixed))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_lr_mixed = model_lr_mixed.predict(X_test_mixed_scaled)\n",
    "print(\"Logistic Regression - Scaled Mixed Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_mixed, y_pred_test_lr_mixed))\n",
    "print(classification_report(y_test_mixed, y_pred_test_lr_mixed))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:19:20.018699Z",
     "start_time": "2024-09-08T19:19:19.285659Z"
    }
   },
   "id": "40d6cd00d04ef610",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Scaled Normal Data - Validation Set Evaluation\n",
      "[[1036    0    5]\n",
      " [   0 3833    0]\n",
      " [   4    1 2342]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "SVM - Scaled Normal Data - Test Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   2    0 2346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Scale the normal data\n",
    "scaler = StandardScaler()\n",
    "X_train_normal_scaled = scaler.fit_transform(X_train_normal)\n",
    "X_val_normal_scaled = scaler.transform(X_val_normal)\n",
    "X_test_normal_scaled = scaler.transform(X_test_normal)\n",
    "\n",
    "# Train SVM on scaled normal data\n",
    "model_svm_normal = SVC(random_state=42)\n",
    "model_svm_normal.fit(X_train_normal_scaled, y_train_normal)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_svm_normal = model_svm_normal.predict(X_val_normal_scaled)\n",
    "print(\"SVM - Scaled Normal Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_normal, y_pred_val_svm_normal))\n",
    "print(classification_report(y_val_normal, y_pred_val_svm_normal))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_svm_normal = model_svm_normal.predict(X_test_normal_scaled)\n",
    "print(\"SVM - Scaled Normal Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_normal, y_pred_test_svm_normal))\n",
    "print(classification_report(y_test_normal, y_pred_test_svm_normal))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:20:22.438354Z",
     "start_time": "2024-09-08T19:20:21.076001Z"
    }
   },
   "id": "6e1623073130fe96",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Scaled Log-Transformed Data - Validation Set Evaluation\n",
      "[[1037    0    4]\n",
      " [   0 3833    0]\n",
      " [   6    0 2341]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "SVM - Scaled Log-Transformed Data - Test Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   4    0 2344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "# Scale the log-transformed data\n",
    "scaler_log = StandardScaler()\n",
    "X_train_log_scaled = scaler_log.fit_transform(X_train_log)\n",
    "X_val_log_scaled = scaler_log.transform(X_val_log)\n",
    "X_test_log_scaled = scaler_log.transform(X_test_log)\n",
    "\n",
    "# Train SVM on scaled log-transformed data\n",
    "model_svm_log = SVC(random_state=42)\n",
    "model_svm_log.fit(X_train_log_scaled, y_train_log)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_svm_log = model_svm_log.predict(X_val_log_scaled)\n",
    "print(\"SVM - Scaled Log-Transformed Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_log, y_pred_val_svm_log))\n",
    "print(classification_report(y_val_log, y_pred_val_svm_log))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_svm_log = model_svm_log.predict(X_test_log_scaled)\n",
    "print(\"SVM - Scaled Log-Transformed Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_log, y_pred_test_svm_log))\n",
    "print(classification_report(y_test_log, y_pred_test_svm_log))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:21:18.560188Z",
     "start_time": "2024-09-08T19:21:17.117065Z"
    }
   },
   "id": "69183218dece1c5e",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Scaled Mixed Data - Validation Set Evaluation\n",
      "[[1037    0    4]\n",
      " [   0 3833    0]\n",
      " [   6    0 2341]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "SVM - Scaled Mixed Data - Test Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   3    0 2345]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "# Scale the mixed data (some features log-transformed, some normal)\n",
    "scaler_mixed = StandardScaler()\n",
    "X_train_mixed_scaled = scaler_mixed.fit_transform(X_train_mixed)\n",
    "X_val_mixed_scaled = scaler_mixed.transform(X_val_mixed)\n",
    "X_test_mixed_scaled = scaler_mixed.transform(X_test_mixed)\n",
    "\n",
    "# Train SVM on scaled mixed data\n",
    "model_svm_mixed = SVC(random_state=42)\n",
    "model_svm_mixed.fit(X_train_mixed_scaled, y_train_mixed)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_svm_mixed = model_svm_mixed.predict(X_val_mixed_scaled)\n",
    "print(\"SVM - Scaled Mixed Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_mixed, y_pred_val_svm_mixed))\n",
    "print(classification_report(y_val_mixed, y_pred_val_svm_mixed))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_svm_mixed = model_svm_mixed.predict(X_test_mixed_scaled)\n",
    "print(\"SVM - Scaled Mixed Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_mixed, y_pred_test_svm_mixed))\n",
    "print(classification_report(y_test_mixed, y_pred_test_svm_mixed))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:21:40.729413Z",
     "start_time": "2024-09-08T19:21:39.302113Z"
    }
   },
   "id": "fc69bf26f85cca95",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:22:57.578252Z",
     "start_time": "2024-09-08T19:22:57.563290Z"
    }
   },
   "id": "9d38bae172d0bfae",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:22:57.811599Z",
     "start_time": "2024-09-08T19:22:57.800681Z"
    }
   },
   "id": "4e8ecb3e95e66630",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN - Scaled Normal Data - Validation Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   3    1 2343]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "KNN - Scaled Normal Data - Test Set Evaluation\n",
      "[[1041    0    0]\n",
      " [   0 3833    0]\n",
      " [   4    0 2344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Scale the normal data\n",
    "scaler = StandardScaler()\n",
    "X_train_normal_scaled = scaler.fit_transform(X_train_normal)\n",
    "X_val_normal_scaled = scaler.transform(X_val_normal)\n",
    "X_test_normal_scaled = scaler.transform(X_test_normal)\n",
    "\n",
    "# Train KNN on scaled normal data\n",
    "model_knn_normal = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\n",
    "model_knn_normal.fit(X_train_normal_scaled, y_train_normal)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_knn_normal = model_knn_normal.predict(X_val_normal_scaled)\n",
    "print(\"KNN - Scaled Normal Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_normal, y_pred_val_knn_normal))\n",
    "print(classification_report(y_val_normal, y_pred_val_knn_normal))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_knn_normal = model_knn_normal.predict(X_test_normal_scaled)\n",
    "print(\"KNN - Scaled Normal Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_normal, y_pred_test_knn_normal))\n",
    "print(classification_report(y_test_normal, y_pred_test_knn_normal))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:23:00.296059Z",
     "start_time": "2024-09-08T19:22:59.044301Z"
    }
   },
   "id": "230c426340b16822",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN - Scaled Log-Transformed Data - Validation Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   4    1 2342]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "KNN - Scaled Log-Transformed Data - Test Set Evaluation\n",
      "[[1041    0    0]\n",
      " [   0 3833    0]\n",
      " [   4    0 2344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "# Scale the log-transformed data\n",
    "scaler_log = StandardScaler()\n",
    "X_train_log_scaled = scaler_log.fit_transform(X_train_log)\n",
    "X_val_log_scaled = scaler_log.transform(X_val_log)\n",
    "X_test_log_scaled = scaler_log.transform(X_test_log)\n",
    "\n",
    "# Train KNN on scaled log-transformed data\n",
    "model_knn_log = KNeighborsClassifier(n_neighbors=5)\n",
    "model_knn_log.fit(X_train_log_scaled, y_train_log)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_knn_log = model_knn_log.predict(X_val_log_scaled)\n",
    "print(\"KNN - Scaled Log-Transformed Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_log, y_pred_val_knn_log))\n",
    "print(classification_report(y_val_log, y_pred_val_knn_log))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_knn_log = model_knn_log.predict(X_test_log_scaled)\n",
    "print(\"KNN - Scaled Log-Transformed Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_log, y_pred_test_knn_log))\n",
    "print(classification_report(y_test_log, y_pred_test_knn_log))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:23:50.674619Z",
     "start_time": "2024-09-08T19:23:49.321240Z"
    }
   },
   "id": "1db49fc99eea07e4",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN - Scaled Mixed Data - Validation Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   3    1 2343]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "KNN - Scaled Mixed Data - Test Set Evaluation\n",
      "[[1041    0    0]\n",
      " [   0 3833    0]\n",
      " [   4    0 2344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "# Scale the mixed data (some features log-transformed, some normal)\n",
    "scaler_mixed = StandardScaler()\n",
    "X_train_mixed_scaled = scaler_mixed.fit_transform(X_train_mixed)\n",
    "X_val_mixed_scaled = scaler_mixed.transform(X_val_mixed)\n",
    "X_test_mixed_scaled = scaler_mixed.transform(X_test_mixed)\n",
    "\n",
    "# Train KNN on scaled mixed data\n",
    "model_knn_mixed = KNeighborsClassifier(n_neighbors=5)\n",
    "model_knn_mixed.fit(X_train_mixed_scaled, y_train_mixed)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_knn_mixed = model_knn_mixed.predict(X_val_mixed_scaled)\n",
    "print(\"KNN - Scaled Mixed Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_mixed, y_pred_val_knn_mixed))\n",
    "print(classification_report(y_val_mixed, y_pred_val_knn_mixed))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_knn_mixed = model_knn_mixed.predict(X_test_mixed_scaled)\n",
    "print(\"KNN - Scaled Mixed Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_mixed, y_pred_test_knn_mixed))\n",
    "print(classification_report(y_test_mixed, y_pred_test_knn_mixed))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:24:17.939913Z",
     "start_time": "2024-09-08T19:24:16.689106Z"
    }
   },
   "id": "f5174f0a297a2410",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 33699, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score -1.936841\n",
      "[LightGBM] [Info] Start training from score -0.633450\n",
      "[LightGBM] [Info] Start training from score -1.123672\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM - Scaled Normal Data - Validation Set Evaluation\n",
      "[[1035    0    6]\n",
      " [   0 3833    0]\n",
      " [   2    0 2345]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "\n",
      "LightGBM - Scaled Normal Data - Test Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   1    0 2347]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Scale the normal data\n",
    "scaler = StandardScaler()\n",
    "X_train_normal_scaled = scaler.fit_transform(X_train_normal)\n",
    "X_val_normal_scaled = scaler.transform(X_val_normal)\n",
    "X_test_normal_scaled = scaler.transform(X_test_normal)\n",
    "\n",
    "# Train LightGBM on scaled normal data\n",
    "model_lgb_normal = lgb.LGBMClassifier(random_state=42)\n",
    "model_lgb_normal.fit(X_train_normal_scaled, y_train_normal)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_lgb_normal = model_lgb_normal.predict(X_val_normal_scaled)\n",
    "print(\"LightGBM - Scaled Normal Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_normal, y_pred_val_lgb_normal))\n",
    "print(classification_report(y_val_normal, y_pred_val_lgb_normal))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_lgb_normal = model_lgb_normal.predict(X_test_normal_scaled)\n",
    "print(\"LightGBM - Scaled Normal Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_normal, y_pred_test_lgb_normal))\n",
    "print(classification_report(y_test_normal, y_pred_test_lgb_normal))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:26:13.361127Z",
     "start_time": "2024-09-08T19:26:11.670650Z"
    }
   },
   "id": "c0e6b152c8eae731",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 33699, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score -1.936841\n",
      "[LightGBM] [Info] Start training from score -0.633450\n",
      "[LightGBM] [Info] Start training from score -1.123672\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM - Scaled Log-Transformed Data - Validation Set Evaluation\n",
      "[[1038    0    3]\n",
      " [   0 3833    0]\n",
      " [   4    0 2343]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "LightGBM - Scaled Log-Transformed Data - Test Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   0    0 2348]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "# Scale the log-transformed data\n",
    "scaler_log = StandardScaler()\n",
    "X_train_log_scaled = scaler_log.fit_transform(X_train_log)\n",
    "X_val_log_scaled = scaler_log.transform(X_val_log)\n",
    "X_test_log_scaled = scaler_log.transform(X_test_log)\n",
    "\n",
    "# Train LightGBM on scaled log-transformed data\n",
    "model_lgb_log = lgb.LGBMClassifier(random_state=42)\n",
    "model_lgb_log.fit(X_train_log_scaled, y_train_log)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_lgb_log = model_lgb_log.predict(X_val_log_scaled)\n",
    "print(\"LightGBM - Scaled Log-Transformed Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_log, y_pred_val_lgb_log))\n",
    "print(classification_report(y_val_log, y_pred_val_lgb_log))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_lgb_log = model_lgb_log.predict(X_test_log_scaled)\n",
    "print(\"LightGBM - Scaled Log-Transformed Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_log, y_pred_test_lgb_log))\n",
    "print(classification_report(y_test_log, y_pred_test_lgb_log))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:27:36.750867Z",
     "start_time": "2024-09-08T19:27:35.922087Z"
    }
   },
   "id": "806b9fa6d3b989ad",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 33699, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score -1.936841\n",
      "[LightGBM] [Info] Start training from score -0.633450\n",
      "[LightGBM] [Info] Start training from score -1.123672\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM - Scaled Mixed Data - Validation Set Evaluation\n",
      "[[1035    0    6]\n",
      " [   0 3833    0]\n",
      " [   3    0 2344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "LightGBM - Scaled Mixed Data - Test Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   0    0 2348]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "# Scale the mixed data (some features log-transformed, some normal)\n",
    "scaler_mixed = StandardScaler()\n",
    "X_train_mixed_scaled = scaler_mixed.fit_transform(X_train_mixed)\n",
    "X_val_mixed_scaled = scaler_mixed.transform(X_val_mixed)\n",
    "X_test_mixed_scaled = scaler_mixed.transform(X_test_mixed)\n",
    "\n",
    "# Train LightGBM on scaled mixed data\n",
    "model_lgb_mixed = lgb.LGBMClassifier(random_state=42)\n",
    "model_lgb_mixed.fit(X_train_mixed_scaled, y_train_mixed)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_lgb_mixed = model_lgb_mixed.predict(X_val_mixed_scaled)\n",
    "print(\"LightGBM - Scaled Mixed Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_mixed, y_pred_val_lgb_mixed))\n",
    "print(classification_report(y_val_mixed, y_pred_val_lgb_mixed))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_lgb_mixed = model_lgb_mixed.predict(X_test_mixed_scaled)\n",
    "print(\"LightGBM - Scaled Mixed Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_mixed, y_pred_test_lgb_mixed))\n",
    "print(classification_report(y_test_mixed, y_pred_test_lgb_mixed))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:28:00.027703Z",
     "start_time": "2024-09-08T19:27:59.174985Z"
    }
   },
   "id": "92632f1504dbdc41",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "57aa2c7463feaab8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b0141d1cfd4aaa67"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Scaled Normal Data - Validation Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   3    0 2344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "\n",
      "XGBoost - Scaled Normal Data - Test Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   2    0 2346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Scale the normal data\n",
    "scaler = StandardScaler()\n",
    "X_train_normal_scaled = scaler.fit_transform(X_train_normal)\n",
    "X_val_normal_scaled = scaler.transform(X_val_normal)\n",
    "X_test_normal_scaled = scaler.transform(X_test_normal)\n",
    "\n",
    "# Train XGBoost on scaled normal data\n",
    "model_xgb_normal = xgb.XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "model_xgb_normal.fit(X_train_normal_scaled, y_train_normal)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_xgb_normal = model_xgb_normal.predict(X_val_normal_scaled)\n",
    "print(\"XGBoost - Scaled Normal Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_normal, y_pred_val_xgb_normal))\n",
    "print(classification_report(y_val_normal, y_pred_val_xgb_normal))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_xgb_normal = model_xgb_normal.predict(X_test_normal_scaled)\n",
    "print(\"XGBoost - Scaled Normal Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_normal, y_pred_test_xgb_normal))\n",
    "print(classification_report(y_test_normal, y_pred_test_xgb_normal))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:33:01.265579Z",
     "start_time": "2024-09-08T19:33:00.214391Z"
    }
   },
   "id": "34ebdb73eaac3fd7",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Scaled Log-Transformed Data - Validation Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   3    0 2344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "\n",
      "XGBoost - Scaled Log-Transformed Data - Test Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   2    0 2346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "# Scale the log-transformed data\n",
    "scaler_log = StandardScaler()\n",
    "X_train_log_scaled = scaler_log.fit_transform(X_train_log)\n",
    "X_val_log_scaled = scaler_log.transform(X_val_log)\n",
    "X_test_log_scaled = scaler_log.transform(X_test_log)\n",
    "\n",
    "# Train XGBoost on scaled log-transformed data\n",
    "model_xgb_log = xgb.XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "model_xgb_log.fit(X_train_log_scaled, y_train_log)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_xgb_log = model_xgb_log.predict(X_val_log_scaled)\n",
    "print(\"XGBoost - Scaled Log-Transformed Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_log, y_pred_val_xgb_log))\n",
    "print(classification_report(y_val_log, y_pred_val_xgb_log))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_xgb_log = model_xgb_log.predict(X_test_log_scaled)\n",
    "print(\"XGBoost - Scaled Log-Transformed Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_log, y_pred_test_xgb_log))\n",
    "print(classification_report(y_test_log, y_pred_test_xgb_log))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:33:38.626020Z",
     "start_time": "2024-09-08T19:33:37.438199Z"
    }
   },
   "id": "75d071ab366617e6",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Scaled Mixed Data - Validation Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   3    0 2344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2347\n",
      "\n",
      "    accuracy                           1.00      7221\n",
      "   macro avg       1.00      1.00      1.00      7221\n",
      "weighted avg       1.00      1.00      1.00      7221\n",
      "\n",
      "XGBoost - Scaled Mixed Data - Test Set Evaluation\n",
      "[[1039    0    2]\n",
      " [   0 3833    0]\n",
      " [   2    0 2346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1041\n",
      "           1       1.00      1.00      1.00      3833\n",
      "           2       1.00      1.00      1.00      2348\n",
      "\n",
      "    accuracy                           1.00      7222\n",
      "   macro avg       1.00      1.00      1.00      7222\n",
      "weighted avg       1.00      1.00      1.00      7222\n"
     ]
    }
   ],
   "source": [
    "# Scale the mixed data (some features log-transformed, some normal)\n",
    "scaler_mixed = StandardScaler()\n",
    "X_train_mixed_scaled = scaler_mixed.fit_transform(X_train_mixed)\n",
    "X_val_mixed_scaled = scaler_mixed.transform(X_val_mixed)\n",
    "X_test_mixed_scaled = scaler_mixed.transform(X_test_mixed)\n",
    "\n",
    "# Train XGBoost on scaled mixed data\n",
    "model_xgb_mixed = xgb.XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "model_xgb_mixed.fit(X_train_mixed_scaled, y_train_mixed)\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "y_pred_val_xgb_mixed = model_xgb_mixed.predict(X_val_mixed_scaled)\n",
    "print(\"XGBoost - Scaled Mixed Data - Validation Set Evaluation\")\n",
    "print(confusion_matrix(y_val_mixed, y_pred_val_xgb_mixed))\n",
    "print(classification_report(y_val_mixed, y_pred_val_xgb_mixed))\n",
    "\n",
    "# Evaluate on Test Set\n",
    "y_pred_test_xgb_mixed = model_xgb_mixed.predict(X_test_mixed_scaled)\n",
    "print(\"XGBoost - Scaled Mixed Data - Test Set Evaluation\")\n",
    "print(confusion_matrix(y_test_mixed, y_pred_test_xgb_mixed))\n",
    "print(classification_report(y_test_mixed, y_pred_test_xgb_mixed))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-08T19:38:23.080670Z",
     "start_time": "2024-09-08T19:38:21.910802Z"
    }
   },
   "id": "f9aef02e424bb008",
   "execution_count": 56
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
